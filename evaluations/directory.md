```
ğŸ“¦evaluations
 â”£ ğŸ“‚assets # from LME
 â”ƒ â”— ğŸ“œlongmemeval_examples.png
 â”£ ğŸ“‚data # full LME test suites
 â”ƒ â”£ ğŸ“œlongmemeval_m_cleaned.json
 â”ƒ â”£ ğŸ“œlongmemeval_oracle.json
 â”ƒ â”— ğŸ“œlongmemeval_s_cleaned.json
 â”£ ğŸ“‚data_ref # raw data files used to generate eval cases
 â”ƒ â”— ğŸ“œBiotechCropsAllTables2024.csv
 â”£ ğŸ“‚src # code
 â”ƒ â”£ ğŸ“‚evaluation # from LME
 â”ƒ â”ƒ â”£ ğŸ“œevaluate_qa.py
 â”ƒ â”ƒ â”£ ğŸ“œprint_qa_metrics.py
 â”ƒ â”ƒ â”— ğŸ“œprint_retrieval_metrics.py
 â”ƒ â”— ğŸ“‚generation # LLM-generated responses
 â”ƒ â”ƒ â”£ ğŸ“œgenerate_one.py
 â”ƒ â”ƒ â”— ğŸ“œgenerate_test.py
 â”£ ğŸ“‚test_json # small eval cases
 â”ƒ â”£ ğŸ“œandrew2_sample.json
 â”ƒ â”£ ğŸ“œandrew_sample.json
 â”ƒ â”— ğŸ“œone.json
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œLICENSE # for LME
 â”£ ğŸ“œREADME.md
 â”£ ğŸ“œdirectory.md
```