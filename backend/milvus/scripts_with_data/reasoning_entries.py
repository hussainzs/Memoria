# synthetically generated Agent lessons to be inserted into Milvus "reasoningbank" collection.
# each object should have "key_lesson", "context_to_prefer", "tags", and optionally "link_nodes" fields. The vectors will be generated through these scripts below.
entries = [
    {
        "key_lesson": "Lesson: Treat elasticity drift as a model governance problem, not a one-off re-fit.\n\nWhen an analyst suspects price-elasticity drift (e.g., after an inflationary episode), the agent should treat the existing elasticity model as a **baseline to be stress-tested**, not discarded. Start from the prior model artifacts (hierarchical coefficients, shrinkage priors, SKU hierarchy) and explicitly compare new fits against that baseline.\n\n1. **Use structured re-fit + diagnostics, not ad-hoc re-estimation.**\n   - Re-fit the hierarchical elasticity model on fresh promo history using rolling windows (e.g., 8-week rolling) so that time variation is captured smoothly rather than in one global jump.\n   - Always run diagnostic passes: sign-flip scan on own- and cross-price elasticities, influence diagnostics, and variance checks on parameter posteriors.\n   - Treat the prior model as an anchor: large deviations from prior coefficients should be explained by data density and event structure (e.g., more extreme promos, competitor shocks), not accepted blindly.\n\n2. **Handle low-volume, sparse-promo SKUs with extra caution.**\n   - Expect that SKUs with low volume and sparse promo history will produce unstable elasticities (wide confidence intervals, higher variance, frequent sign flips on cross-price terms).\n   - When the diagnostic layer flags high-variance tails clustered in low-volume SKUs, the agent should **downweight these estimates in decision-making** and avoid aggressive repricing solely on the basis of noisy coefficients.\n\n3. **Differentiate between trip-drivers and long tail when acting on drift.**\n   - Trip-driver SKUs (high-traffic items) should have more conservative thresholds for action: require both consistent drift (across windows) and tight confidence intervals before recommending price changes.\n   - For long-tail SKUs that show steepened own-price elasticity or cross-price sign flips but with noisy estimates, the preferred action is **targeted learning (controlled pilots)** rather than broad permanent price moves.\n\n4. **Translate diagnostics into explicit operational flags.**\n   - As part of the re-fit output, the agent should generate per-SKU flags such as `own_steepened_flag`, `cross_sign_flip_flag`, `variance_flag`, and `pilot_candidate_flag`, in addition to the raw elasticities and confidence intervals.\n   - The recommendation layer should then operate on these flags (e.g., “37 SKUs with `own_steepened_flag=1`, 11 SKUs with `cross_sign_flip_flag=1`, 12 SKUs with `pilot_candidate_flag=1`), rather than on raw coefficients alone.\n\n5. **Default path: hold on critical SKUs, learn on flagged SKUs.**\n   - When diagnostics reveal drift but uncertainty remains, the safe default is: hold prices on key trip-drivers, design controlled pilots for a small, carefully chosen set of flagged SKUs, and only consider broader repricing after pilot evidence accumulates.\n\nApplied to the Walmart context, this discipline explains why the agent re-fit the hierarchical model using Jan–Apr promo history, highlighted 37 SKUs with steepened own-price elasticity and 11 with cross-price sign flips, and recommended holding prices on trip-drivers while scheduling controlled pilots for 12 flagged SKUs instead of immediately repricing at scale.",
        "context_to_prefer": "Use this lesson whenever the agent is asked to:\n- Re-fit or re-estimate **price elasticity models** after macro shocks (inflation, recession, stimulus) or major strategy changes.\n- Diagnose potential **elasticity drift** using new promo history or transaction logs.\n- Compare a **new elasticity model** against an older baseline (hierarchical Bayesian pricing model, demand model with shrinkage priors, SKU hierarchy).\n- Decide how to act on findings such as **steepened own-price elasticity**, **cross-price sign flips**, or **high-variance parameters**.\n- Decide between **broad repricing** vs. **limited pilots** for a subset of SKUs.\n\nRelevant keywords and cues: price elasticity, elasticity drift, hierarchical regression, Bayesian shrinkage, promo history, discount depth, rolling window, own-price elasticity, cross-price elasticity, sign flip, variance diagnostics, influence diagnostics, low-volume SKUs, sparse promo history, trip-driver SKUs, long tail assortment, pilot candidates, controlled experiments, pricing governance, model governance, drift detection, Walmart pricing, retail price optimization.",
        "tags": [
            "pricing",
            "elasticity_drift",
            "hierarchical_model",
            "model_governance",
            "promo_history",
            "trip_drivers",
            "pilot_design",
            "retail_analytics",
        ],
        "link_nodes": ["N3101", "N3102", "N3103", "N3104", "N3105"],
    },
    {
        "key_lesson": "Lesson: Reuse elasticity outputs and segmentation to design guardrailed pilots and bounded competitor responses.\n\nOnce elasticity drift has been quantified, the agent should **reuse those elasticity outputs as structured priors** for both proactive pilots and reactive competitor-shock responses, always under explicit guardrails.\n\n1. **Design targeted pilots with ex-ante guardrails and analysis plans.**\n   - Combine elasticity outputs (own- and cross-price elasticities, sign-flip and variance flags) with customer/store segmentation (income proxy, price sensitivity, basket mix, store clusters) to select **who** should be treated.\n   - Prefer randomization at the cluster level (e.g., 64 store clusters → 32 treatment, 32 control) rather than at individual-customer level when operationally simpler and less prone to interference.\n   - Predefine a modest discount (e.g., 5%) targeted at **high-sensitivity segments in low-income geographies**, and simulate uplift and cannibalization across P10/P50/P90 scenarios before launching.\n   - Lock in the analysis framework up front (e.g., CUPED and fixed-effects models) and formalize numeric **stop/scale rules**, such as: stop if GM$ delta < −0.5% or stockouts > 2%; scale if GM$ ≥ +0.5% and service risk is acceptable.\n\n2. **Treat competitor shocks as experiments under pressure, not excuses to abandon guardrails.**\n   - When a competitor abruptly drops prices (e.g., −7% median on leader SKUs), pull in cross-price elasticities from the refit results and competitor price data (including reversion probabilities) from the market intel feed.\n   - Simulate multiple response options—temporary discounts, bundles, loyalty credits—under the same margin and inventory guardrails used in prior pilots.\n   - Prefer **time-bounded, reversible plays** (e.g., 2-week bundle + loyalty credit targeted to high cross-price clusters) over broad price-matching that is hard to roll back and can destroy margin.\n   - Use reversion probabilities from the intel feed to justify **auto-rollback criteria** (e.g., revert when competitor prices normalize or at a fixed end date such as 2025-11-26).\n\n3. **Maintain a consistent guardrail language across pilots and shocks.**\n   - Represent outcomes in a common metric set: expected lift %, GM$ or GM% impact, cannibalization rate, and stockout or service-risk thresholds.\n   - Ensure the recommendation is framed in this guardrail language (e.g., “expected lift +6.8% (P50), cannibalization <1.2%, GM$ neutral to +0.3%; stop if GM$ < −0.5% or stockouts >2%; scale if GM$ ≥ +0.5%”).\n   - Reuse this template even under time pressure from competitor actions so that decision-makers can compare options quickly and reliably.\n\n4. **Link artifacts across steps instead of redoing work.**\n   - Treat the elasticity refit output (per-SKU elasticities and flags) as a reusable artifact that feeds into both discount pilots and competitor responses.\n   - Use the same cluster definitions and segmentation table across experiments so that learnings from one pilot (e.g., cluster-level response patterns) can immediately inform later shock responses.\n\nIn the Walmart flows, this logic explains why the agent: (i) used the elasticity refit results plus segmentation to design a 5% discount pilot with 32 treatment and 32 control clusters, clear revenue-neutral guardrails, and CUPED/FE analysis; and (ii) later, when a competitor dropped prices on leader SKUs, reused cross-price elasticities and guardrail logic to recommend a bounded 2-week bundle + $3 loyalty credit with explicit rollback rules instead of a broad price match.",
        "context_to_prefer": "Use this lesson whenever the agent is asked to:\n- Design or evaluate **discount pilots** or **A/B tests** using pre-computed elasticity estimates and customer segmentation.\n- Plan **targeted promotions** for high price-sensitivity or low-income segments while keeping overall GM$ impact near neutral.\n- Define **stop/scale rules** and guardrails for pilots (GM$, GM%, stockouts, service risk) before launch.\n- Respond to **competitor price shocks** (sudden competitor price cuts on leader SKUs) using cross-price elasticities and market-intel feeds.\n- Choose between response options such as temporary discounts, bundles, loyalty credits, and broad price matching.\n- Set or reuse common metrics and thresholds across experiments: expected lift, cannibalization, GM$ delta, stockout thresholds, rollback dates or triggers.\n\nRelevant keywords and cues: targeted discount pilot, A/B testing, CUPED, fixed effects, high-sensitivity segments, low-income geographies, price-sensitivity segmentation, store clusters, revenue-neutral promotions, guardrail policy, GM$ guardrail, stockout threshold, cannibalization, elasticity priors, cross-price elasticity, competitor price shock, competitor price drop, bundle offer, loyalty credit, rollback criteria, market intel feed, reversion probability, Walmart pricing, retail experimentation framework.",
        "tags": [
            "ab_testing",
            "pilot_design",
            "guardrails",
            "discount_strategy",
            "competitor_response",
            "cross_price_elasticity",
            "segmentation",
            "retail_experimentation",
        ],
        "link_nodes": [
            "N3201",
            "N3202",
            "N3203",
            "N3204",
            "N3205",
            "N3301",
            "N3302",
            "N3303",
            "N3304",
        ],
    },
    {
        "key_lesson": "Censored-demand correction + hierarchical forecasting with exogenous weather & promotion features to avoid under-estimating demand and mis-setting safety stock",
        "context_to_prefer": "stockout, censored_demand, hierarchical_forecast, safety_stock, P90, fill_rate 0.97, Bayesian_shrinkage, SKU>Category>StoreCluster, weather_features (MaxTemp, HeatIndex, MA7), promo_flags, lost_sales_estimation, on_hand==0 vs missing, forecast_bias, demand_uplift, regional_heatwave, display_promos, transfer_plan, cross_dock, inventory_policy_update, southeast, cold_brew, iced_coffee, short_term_anomaly",
        "tags": [
            "censored_demand",
            "hierarchical_modeling",
            "weather_as_regressor",
            "promo_interaction",
            "safety_stock_recalc",
            "bayesian_shrinkage",
            "lost_sales",
            "inventory_policy",
        ],
        "link_nodes": ["N2002", "N2003", "N2004", "N2005", "N2006", "N2007"],
        "text": "When observed sales contain stockout periods (On_Hand == 0), treat zeros as potentially censored, not as true zero demand. Estimate lost sales with a shrinkage estimator that pools information across SKU→category→storeCluster (hierarchical Bayesian or empirical Bayes) to avoid extreme local estimates from sparse stores. Augment models with exogenous regressors that are likely causal for short anomalies — e.g., MaxTemp, HeatIndex, MA7 moving averages, and promo dummies (FeatureFlag, DisplayFlag, DiscountPct) — and include interaction terms (weather × promo) because promotions can amplify a weather-driven uplift. Refit hierarchical demand model using the censored-observation likelihood (or impute lost sales before refit). Propagate the posterior predictive distribution into inventory policy: compute P10/P50/P90 demand bands and derive safety stock from the P90 forecast conditional on a specified target fill rate (example used: target_fill_rate=0.97). Practical checks & pitfalls: (1) confirm On_Hand==0 is inventory-out, not missing data or a reporting cutoff; (2) align calendar and timezone across POS, promo, and weather feeds; (3) verify promo flags represent actual in-store displays (Feature/Display) — electronic promo records sometimes over-report; (4) quantify uncertainty from censorship imputation — do not collapse to a single point forecast when changing safety stock; (5) confirm lead times & DC constraints before recommending transfers; (6) guard against double-counting uplift when both weather and promo signals exist. When applied correctly this prevents under-provisioning (stockouts) during transitory demand shocks and yields defensible safety-stock and reorder point updates that factor in both mean uplift and tail risk.",
    },
    {
        "key_lesson": "Operationally-feasible uplift allocation and pilot design: combine multi-year seasonal priors with DC constraints and confidence bands; protect destination/trip-driver SKUs and avoid confounded windows when simulating SKU changes",
        "context_to_prefer": "holiday_uplift, seasonality_detection, T-14..T+3 window, P10 P50 P90, allocation_prepull, DC_capacity, trailer_slots, carrier_cutoffs, labor_windows, ops_feasibility, GM$_tradeoffs, pilot_design, A/B pilots, store_level_simulation, substitute_stockout_risk, destination_SKUs, trip_driver, holiday_window (Thanksgiving), confounded_periods (BlackFriday), knowledge_transfer (weather→holiday), readouts, halt_conditions",
        "tags": [
            "seasonal_uplift",
            "ops_feasibility",
            "confidence_bands",
            "allocation_planning",
            "pilot_design",
            "sku_rationalization",
            "risk_controls",
        ],
        "link_nodes": [
            "N2011",
            "N2012",
            "N2013",
            "N2014",
            "N2015",
            "N2022",
            "N2023",
            "N2024",
            "N2025",
        ],
        "text": "When recommending allocation shifts or pre-pulls for predictable seasonal uplifts (e.g., Thanksgiving window), derive seasonal shape and amplitude from multiple prior years (preferably ≥3) using event windows (example: T-14..T+3) and priors (Fourier + event dummies). Translate statistical bands (P10/P50/P90) into operational actions only after checking DC and carrier constraints: compute feasible pre-pull volumes per DC by respecting max pick capacity, trailer slot limits, carrier cutoffs, and labor windows. Produce explicit GM$ trade-offs: for each allocation scenario estimate expected fill-rate change and incremental GM$ (net of pull & transport costs). For SKU-rationalization and pilot designs: (1) compute composite scores that combine margin × demand stability − (space + handling) − substitutability_penalty; (2) identify and protect destination/trip-driver SKUs regardless of composite score; (3) simulate cross-elasticities using historical price/promo shocks to estimate revenue leakage to substitutes; (4) schedule pilots in non-confounded windows (avoid Black Friday or other campaign weeks) and establish weekly KPI readouts and explicit halt conditions (stockout thresholds, shopper feedback flags, adverse GM$ swing). Transfer lessons across contexts: e.g., short heatwave lessons (censored-demand, safety stock) should tighten guardrails for delist pilots to avoid substitute stockouts. Key failure modes to store: ignoring ops constraints (leads to infeasible pre-pulls), using single-year seasonality (overfits anomalous years), failing to monitor substitute stockouts during pilots, and performing pilots during confounded promotional windows.",
    },
    {
        "key_lesson": 'When reacting to logistics disruptions (e.g., port congestion or simulated hurricanes), an AI agent must treat ETA and routing as a constrained optimization problem rather than a simple shortest-time reroute.\n\nIn the port congestion case, the agent correctly combined an external port telemetry feed with an internal PO/container ETA snapshot. The external JSON feed exposed queue length, berth delays, and dwell time distributions by port, while the internal snapshot held container-level ETAs, DC destinations, and product mix. Instead of using raw or average delays, the agent explicitly recalculated ETAs using seasonal P75/P90 percentiles for the congested port and candidate alternative ports. This quantile-based approach reduced the risk of over-optimistic ETAs during peak season and made the comparison between staying at the congested port versus re-routing more robust.\n\nAt the same time, the agent respected hard operational constraints. It checked DC receiving slots, yard capacity, linehaul capacity, labor shifts, and HOS rules before recommending re-routing. Only the subset of containers that satisfied these constraints and produced a meaningful ETA gain were considered for diversion. This filtered the candidate set down to 28 containers and ensured the recommendation was operationally executable instead of purely theoretical.\n\nCritically, the agent did not treat all freight as equal. It prioritized low-substitutability SKUs (e.g., specific electronics and accessories with high holiday uplift and limited alternatives) and estimated gross margin dollars at risk if those items were delayed. The final recommendation balanced ETA improvement (+2.5 days median), GM dollars protected (~$1.8M), and incremental transportation cost (~$120k). The decision rule can be summarized as: "Protect the SKUs with high margin and low substitutability first, provided rerouting remains within feasible capacity and regulatory constraints, and the GM protected significantly exceeds incremental logistics cost."\n\nIn the subsequent disaster-response drill, the agent reused these learned heuristics on synthetic hurricane scenario data. It disabled affected coastal DCs, re-routed through inland hubs, recomputed feasible lanes under HOS and fuel surcharges, and explicitly shielded top-decile demand items. The drill showed that the same principles—quantile-aware ETA modeling, constraint-aware routing, and prioritization of critical SKUs—generalize beyond a single incident. It also surfaced automation gaps (e.g., missing dynamic yard slotting rules, carrier swap rules), reminding us that good reasoning must be paired with instrumentation to capture and close execution gaps over time.\n\nOverall, the lesson is: treat disruption routing as a risk-aware, constraint-aware optimization problem that simultaneously:\n- Uses robust delay distributions (quantiles, seasonal percentiles) instead of naive averages.\n- Joins external telemetry with internal PO/container and DC data.\n- Enforces capacity and regulatory constraints before recommending reroutes.\n- Prioritizes low-substitutability, high-value SKUs and quantifies GM dollars protected versus incremental logistics cost.\n- Reuses these heuristics in drills and scenario planning to test resilience and identify automation gaps.',
        "context_to_prefer": "Use this lesson when the AI agent is handling supply chain disruption analysis, port congestion triage, disaster or hurricane what-if simulations, logistics routing optimization, or DC reallocation scenarios. It is especially applicable when:\n- Joining external port or lane telemetry (queue length, berth delays, dwell times) with internal PO/container ETA data.\n- Evaluating re-routing options across multiple ports or hubs under DC receiving slot limits, yard space constraints, linehaul capacity, labor shifts, or HOS rules.\n- Deciding how many containers or shipments to reroute during holiday peaks or other demand surges.\n- Prioritizing low-substitutability or high holiday uplift SKUs, and quantifying GM dollars at risk versus incremental ocean/linehaul cost.\n- Designing or running resilience drills and disaster-response playbooks that require shielding top-decile demand items and measuring KPIs such as fill-rate, lane feasibility percent, time to first plan, and GM protected.\nInclude this reasoning bank entry for tasks tagged with supply_chain, port_congestion, routing_optimization, capacity_planning, disaster_response, what_if_scenario, holiday_peak, DC_capacity, ETA_modeling, or resilience_drill.",
        "tags": [
            "supply_chain",
            "port_congestion",
            "routing_optimization",
            "eta_quantiles",
            "capacity_constraints",
            "dc_slots",
            "linehaul_capacity",
            "holiday_peak",
            "gm_protection",
            "disaster_response",
            "resilience_drill",
            "what_if_planning",
            "sku_substitutability",
            "logistics_risk_management",
            "automation_gaps",
        ],
        "link_nodes": [
            "N2101",
            "N2102",
            "N2103",
            "N2104",
            "N2105",
            "N2301",
            "N2302",
            "N2303",
            "N2304",
        ],
    },
    {
        "key_lesson": 'When recommending supplier changes or pricing actions, an AI agent must integrate reliability-adjusted cost modeling with robust statistical diagnostics and avoid overreacting to noisy signals.\n\nIn the supplier simulation, the agent did not simply pick the cheapest vendor. It used a mixed-integer program that minimized reliability-adjusted landed cost while enforcing a target service level (≥95%), DC capacity limits, and minimum allocation per supplier. Supplier performance data included lead time, on-time rate, lead time variance, defect rate, and region/port attributes. The optimization penalized sourcing plans that concentrated volume in suppliers with high delay variance or strongly correlated disruption risk. As a result, the agent recommended shifting 35–45% of volume away from Supplier #342 (chronically late, high variance) to Suppliers #219 and #455, which have better reliability and uncorrelated delay shocks (different ports/weather basins). This dropped expected late_rate from ~18% to ~9% while increasing landed cost only marginally (+$0.04 per unit) and improving service-level to ~96.2%.\n\nA key reasoning pattern here is that reliability and diversification matter as much as unit cost. The agent explicitly modeled covariance of delay across suppliers and used a risk heatmap to visualize correlated shocks. Under this lens, a supplier with slightly higher unit cost but lower variance and different disruption drivers can improve fleet-wide resilience and service-level. The sourcing split output should therefore be interpreted as a portfolio allocation: avoid single-supplier dependence, respect capacity constraints, and treat correlated delay risk as a first-class optimization term, not an afterthought.\n\nIn the elasticity drift analysis, the agent revisited price elasticities using fresh promo and competitor data. It re-fit a hierarchical model with rolling windows, then ran diagnostics such as sign-flip scans and influence checks. The findings showed that 37 SKUs had steepened own-price elasticity and 11 SKUs exhibited cross-price sign flips versus a key competitor. Crucially, the agent recognized that high-variance tails clustered in low-volume SKUs with sparse and overlapping promos. Instead of blindly repricing all SKUs based on unstable estimates, it recommended:\n- Holding prices on "trip-driver" SKUs (high-volume, strategic items).\n- Running controlled pilots for a subset of 12 flagged SKUs to validate the new elasticities before broad roll-out.\n\nThe execution lesson is that model diagnostics and data quality must gate large-scale actions. Low-volume, sparsely-promoted SKUs are prone to noisy elasticity estimates and sign flips; any automated repricing strategy should mark those as "experiment first" rather than immediately pushing price changes.\n\nOverall, the lesson is: combine reliability-aware sourcing optimization with statistically disciplined pricing diagnostics. Use risk diversification and covariance penalties when changing suppliers, and use diagnostics, variance checks, and controlled pilots when re-fitting elasticity models, especially for low-volume SKUs. Avoid recommendations that chase small cost savings or statistically fragile coefficients at the expense of service-level and robustness.',
        "context_to_prefer": 'Use this lesson when the AI agent is asked to:\n- Recommend supplier switches, reallocations, or sourcing splits based on scorecards that include lead time, on-time rate, delay variance, defect rate, region/port, and rebates.\n- Balance landed cost against service-level targets (e.g., ≥95%) and DC capacity constraints.\n- Model reliability-adjusted cost, including risk penalties based on covariance of delays between suppliers, and visualize results in risk heatmaps.\n- Reduce late_rate while keeping unit cost increases small, or design diversification strategies across suppliers, ports, and weather basins.\n- Re-fit price elasticity models using promo history and competitor prices; scan for sign flips, unstable cross-price effects, or high-variance coefficients.\n- Decide whether to hold prices on strategic "trip-driver" SKUs or run limited pilots on a small set of flagged SKUs instead of system-wide repricing.\nInclude this reasoning bank entry for tasks tagged with supplier_optimization, sourcing_split, reliability_adjusted_cost, service_level, risk_diversification, covariance_penalty, pricing_analytics, elasticity_drift, promo_history, competitor_analysis, variance_diagnostics, or experimentation_policy.',
        "tags": [
            "supplier_optimization",
            "reliability_adjusted_cost",
            "service_level",
            "late_rate_reduction",
            "risk_diversification",
            "covariance_penalty",
            "supplier_scorecard",
            "sourcing_split",
            "pricing_analytics",
            "elasticity_drift",
            "promo_history",
            "model_diagnostics",
            "sign_flip_detection",
            "trip_drivers",
            "controlled_experiments",
        ],
        "link_nodes": [
            "N2201",
            "N2202",
            "N2203",
            "N2204",
            "N3101",
            "N3102",
            "N3103",
            "N3104",
            "N3105",
        ],
    },
    {
        "key_lesson": "When a user requests a 'manager-ready' deck or provides specific stylistic feedback (e.g., 'dark theme', 'clean tables', 'action owners'), these preferences must be prioritized over default generation templates. This is a common and high-value revision pattern. The agent's initial 'standard' output (v1) often serves as a baseline for this stylistic alignment. The agent must correctly map qualitative user feedback (like 'clean tables over pie charts') to specific tool parameters (e.g., updating chart types from 'pie' to 'bar' and 'table'). Always confirm if accountability fields like 'action owners' and 'due dates' need to be added as explicit callouts on relevant slides, as this is a frequent requirement for management-level summaries.",
        "context_to_prefer": "User requests for revisions on generated reports, presentations, decks, or scorecards. Scenarios involving stylistic preferences, manager-ready outputs, executive summaries, dark themes, light themes, chart preferences (pie vs. bar vs. table), and adding accountability fields like 'action owners' or 'due dates' to a deliverable. Use when a user asks to 'package' an analysis for an executive or manager.",
        "tags": [
            "presentation",
            "report_generation",
            "user_preference",
            "style_alignment",
            "revision",
            "deck_builder",
            "executive_summary",
        ],
        "link_nodes": ["N5011", "N5014", "N5015", "N5016"],
    },
    {
        "key_lesson": "When performing sustainability audits (e.g., CO₂ footprint, tCO₂e) or other optimization tasks, the analysis must extend beyond simple calculation and ranking. The core task is often multi-objective constrained optimization. User requests to find 'greener' or 'cheaper' substitutes must be balanced against implicit or explicit business constraints like 'service levels' (e.g., >=95%), 'lead-time variance' (e.g., <=2 days), or 'P&L neutrality.' The agent's reasoning must integrate these constraints into the substitution analysis, not just use them as a final filter. The final recommendation must quantify the trade-offs and impact (e.g., '17% tCO₂e reduction with neutral P&L').",
        "context_to_prefer": "Sustainability analysis, carbon footprint calculation, CO2 audit, tCO2e per SKU-mile, supplier substitution, green logistics, multi-objective optimization, constrained optimization. Use when a user asks to find alternatives (e.g., 'greener substitutes,' 'cheaper suppliers') while also mentioning operational metrics (e.g., 'service levels,' 'lead time,' 'cost').",
        "tags": [
            "sustainability",
            "co2",
            "optimization",
            "constraints",
            "service_level",
            "recommendation",
            "multi-objective",
        ],
        "link_nodes": ["N5021", "N5024", "N5025"],
    },
]
